{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Acknowledgements\" data-toc-modified-id=\"Acknowledgements-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Acknowledgements</a></span></li><li><span><a href=\"#Prepare-data-and-model\" data-toc-modified-id=\"Prepare-data-and-model-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Prepare data and model</a></span></li><li><span><a href=\"#Review-segmentation\" data-toc-modified-id=\"Review-segmentation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Review segmentation</a></span></li><li><span><a href=\"#Customer-segmentation-using-Word2Vec-without-metadata\" data-toc-modified-id=\"Customer-segmentation-using-Word2Vec-without-metadata-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Customer segmentation using Word2Vec without metadata</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-27T04:47:17.954Z"
    }
   },
   "source": [
    "# Acknowledgements\n",
    "Thanks to the tutorial: https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:48:56.129341Z",
     "start_time": "2019-11-27T06:48:49.880594Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daviderickson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import nltk.data\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:48:57.369318Z",
     "start_time": "2019-11-27T06:48:56.134682Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_reviews(size='small'): \n",
    "    if size == 'small':\n",
    "        filename = r'../../data/small-review.json'\n",
    "    elif size == 'intermediate':\n",
    "        filename = r'../../data/intermediate-review.json'\n",
    "    elif size == 'full':\n",
    "        filename = r'../../data/review.json'\n",
    "    new_list = []\n",
    "    for line in open(filename):\n",
    "       new_list.append(json.loads(line))\n",
    "    return pd.DataFrame.from_records(new_list)\n",
    "\n",
    "dfreviews = load_reviews(size='intermediate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:48:57.392973Z",
     "start_time": "2019-11-27T06:48:57.372168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ujmEBvifdJM6h6RLv4wQIg</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-05-07 04:34:36</td>\n",
       "      <td>1</td>\n",
       "      <td>Q1sbwvVQXV2734tPgoKj4Q</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Total bill for this horrible service? Over $8G...</td>\n",
       "      <td>6</td>\n",
       "      <td>hG7b0MtEbXx5QzbzE6C_VA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NZnhc2sEQy3RmzKTZnqtwQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-14 21:30:33</td>\n",
       "      <td>0</td>\n",
       "      <td>GJXCdrto3ASJOqKeVWPi6Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I *adore* Travis at the Hard Rock's new Kelly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>yXQM5uF2jS6es16SJzNHfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WTqjgwHlXbSFevF32_DJVw</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-09 20:09:03</td>\n",
       "      <td>0</td>\n",
       "      <td>2TzJjDVDEuAW6MR5Vuc1ug</td>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say that this office really has it t...</td>\n",
       "      <td>3</td>\n",
       "      <td>n6-Gk65cPZL6Uz8qRm3NYw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ikCg8xy5JIg_NGPx-MSIDA</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-09 20:56:38</td>\n",
       "      <td>0</td>\n",
       "      <td>yi0R0Ugj_xUx_Nek0-_Qig</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Went in for a lunch. Steak sandwich was delici...</td>\n",
       "      <td>0</td>\n",
       "      <td>dacAIZ6fTM6mqwW5uxkskg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b1b1eb3uo-w561D0ZfCEiQ</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-30 23:07:38</td>\n",
       "      <td>0</td>\n",
       "      <td>11a8sVPMUFtaC7_ABRkmtw</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Today was my second out of three sessions I ha...</td>\n",
       "      <td>7</td>\n",
       "      <td>ssoyf2_x0EQMed6fgHeMyQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool                 date  funny  \\\n",
       "0  ujmEBvifdJM6h6RLv4wQIg     0  2013-05-07 04:34:36      1   \n",
       "1  NZnhc2sEQy3RmzKTZnqtwQ     0  2017-01-14 21:30:33      0   \n",
       "2  WTqjgwHlXbSFevF32_DJVw     0  2016-11-09 20:09:03      0   \n",
       "3  ikCg8xy5JIg_NGPx-MSIDA     0  2018-01-09 20:56:38      0   \n",
       "4  b1b1eb3uo-w561D0ZfCEiQ     0  2018-01-30 23:07:38      0   \n",
       "\n",
       "                review_id  stars  \\\n",
       "0  Q1sbwvVQXV2734tPgoKj4Q    1.0   \n",
       "1  GJXCdrto3ASJOqKeVWPi6Q    5.0   \n",
       "2  2TzJjDVDEuAW6MR5Vuc1ug    5.0   \n",
       "3  yi0R0Ugj_xUx_Nek0-_Qig    5.0   \n",
       "4  11a8sVPMUFtaC7_ABRkmtw    1.0   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  Total bill for this horrible service? Over $8G...       6   \n",
       "1  I *adore* Travis at the Hard Rock's new Kelly ...       0   \n",
       "2  I have to say that this office really has it t...       3   \n",
       "3  Went in for a lunch. Steak sandwich was delici...       0   \n",
       "4  Today was my second out of three sessions I ha...       7   \n",
       "\n",
       "                  user_id  \n",
       "0  hG7b0MtEbXx5QzbzE6C_VA  \n",
       "1  yXQM5uF2jS6es16SJzNHfg  \n",
       "2  n6-Gk65cPZL6Uz8qRm3NYw  \n",
       "3  dacAIZ6fTM6mqwW5uxkskg  \n",
       "4  ssoyf2_x0EQMed6fgHeMyQ  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:48:57.401895Z",
     "start_time": "2019-11-27T06:48:57.397288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text',\n",
       "       'useful', 'user_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreviews.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:48:57.412642Z",
     "start_time": "2019-11-27T06:48:57.404228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total bill for this horrible service? Over $8Gs. These crooks actually had the nerve to charge us $69 for 3 pills. I checked online the pills can be had for 19 cents EACH! Avoid Hospital ERs at all costs.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreviews['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:49:00.779424Z",
     "start_time": "2019-11-27T06:48:57.415372Z"
    }
   },
   "outputs": [],
   "source": [
    "# For simplicity, drop anything that isn't a letter\n",
    "# Numbers and symbols may have interesting meaning and could be explore later\n",
    "\n",
    "def lettersOnly(string):\n",
    "    return re.sub(\"[^a-zA-Z]\", \" \", string) \n",
    "\n",
    "dfreviews['text'] = dfreviews['text'].apply(lettersOnly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:49:00.786395Z",
     "start_time": "2019-11-27T06:49:00.781171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total bill for this horrible service  Over   Gs  These crooks actually had the nerve to charge us     for   pills  I checked online the pills can be had for    cents EACH  Avoid Hospital ERs at all costs '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfreviews['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:49:00.798189Z",
     "start_time": "2019-11-27T06:49:00.791405Z"
    }
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist(string, remove_stopwords=False):\n",
    "    string = re.sub(\"[^a-zA-Z]\", \" \", string) # keep only letters. more complex model possible later\n",
    "    words =  string.lower().split() # make everything lowercase. split into words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words('english')) # create a fast lookup for stopwords\n",
    "        words = [w for w in words if not w in stops] # remove stopwords\n",
    "    return( words) # return a list of words\n",
    "    \n",
    "# dfreviews['text'] = dfreviews['text'].apply(review_to_words) # apply to reviews in dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:49:00.827967Z",
     "start_time": "2019-11-27T06:49:00.802865Z"
    }
   },
   "outputs": [],
   "source": [
    "# Word2Vec expects single sentences, each one as a list of words\n",
    "\n",
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "# Define a function to split a review into parsed sentences\n",
    "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
    "    # Function to split a review into parsed sentences. Returns a \n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append( review_to_wordlist( raw_sentence, \\\n",
    "              remove_stopwords ))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:49:11.532185Z",
     "start_time": "2019-11-27T06:49:00.830348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences\n"
     ]
    }
   ],
   "source": [
    "sentences = []  # Initialize an empty list of sentences\n",
    "\n",
    "print(\"Parsing sentences\")\n",
    "for review in dfreviews[\"text\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:50:05.942118Z",
     "start_time": "2019-11-27T06:49:11.533678Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 22:49:12,366 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-11-26 22:49:12,379 : INFO : collecting all words and their counts\n",
      "2019-11-26 22:49:12,379 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-11-26 22:49:12,565 : INFO : PROGRESS: at sentence #10000, processed 1088334 words, keeping 25539 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 22:49:12,765 : INFO : PROGRESS: at sentence #20000, processed 2172597 words, keeping 35463 word types\n",
      "2019-11-26 22:49:12,943 : INFO : PROGRESS: at sentence #30000, processed 3251616 words, keeping 42649 word types\n",
      "2019-11-26 22:49:13,131 : INFO : PROGRESS: at sentence #40000, processed 4373996 words, keeping 48893 word types\n",
      "2019-11-26 22:49:13,328 : INFO : PROGRESS: at sentence #50000, processed 5471587 words, keeping 53964 word types\n",
      "2019-11-26 22:49:13,522 : INFO : PROGRESS: at sentence #60000, processed 6570064 words, keeping 58362 word types\n",
      "2019-11-26 22:49:13,714 : INFO : PROGRESS: at sentence #70000, processed 7667364 words, keeping 62704 word types\n",
      "2019-11-26 22:49:13,896 : INFO : PROGRESS: at sentence #80000, processed 8768955 words, keeping 66443 word types\n",
      "2019-11-26 22:49:14,078 : INFO : PROGRESS: at sentence #90000, processed 9872097 words, keeping 70199 word types\n",
      "2019-11-26 22:49:14,266 : INFO : collected 73717 word types from a corpus of 10978770 raw words and 99987 sentences\n",
      "2019-11-26 22:49:14,267 : INFO : Loading a fresh vocabulary\n",
      "2019-11-26 22:49:14,312 : INFO : effective_min_count=40 retains 8557 unique words (11% of original 73717, drops 65160)\n",
      "2019-11-26 22:49:14,314 : INFO : effective_min_count=40 leaves 10670794 word corpus (97% of original 10978770, drops 307976)\n",
      "2019-11-26 22:49:14,343 : INFO : deleting the raw counts dictionary of 73717 items\n",
      "2019-11-26 22:49:14,346 : INFO : sample=0.001 downsamples 57 most-common words\n",
      "2019-11-26 22:49:14,349 : INFO : downsampling leaves estimated 7804072 word corpus (73.1% of prior 10670794)\n",
      "2019-11-26 22:49:14,377 : INFO : estimated required memory for 8557 words and 300 dimensions: 24815300 bytes\n",
      "2019-11-26 22:49:14,378 : INFO : resetting layer weights\n",
      "2019-11-26 22:49:14,504 : INFO : training model with 4 workers on 8557 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-11-26 22:49:15,511 : INFO : EPOCH 1 - PROGRESS: at 10.11% examples, 780739 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:16,516 : INFO : EPOCH 1 - PROGRESS: at 20.61% examples, 793130 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:17,517 : INFO : EPOCH 1 - PROGRESS: at 30.60% examples, 784794 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:18,532 : INFO : EPOCH 1 - PROGRESS: at 40.38% examples, 780805 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:19,541 : INFO : EPOCH 1 - PROGRESS: at 50.61% examples, 782127 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:20,549 : INFO : EPOCH 1 - PROGRESS: at 60.41% examples, 778694 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:21,552 : INFO : EPOCH 1 - PROGRESS: at 70.24% examples, 776714 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:22,553 : INFO : EPOCH 1 - PROGRESS: at 79.96% examples, 774541 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:23,556 : INFO : EPOCH 1 - PROGRESS: at 89.91% examples, 774864 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:24,563 : INFO : EPOCH 1 - PROGRESS: at 99.59% examples, 772715 words/s, in_qsize 5, out_qsize 0\n",
      "2019-11-26 22:49:24,575 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-26 22:49:24,590 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 22:49:24,604 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 22:49:24,611 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 22:49:24,612 : INFO : EPOCH - 1 : training on 10978770 raw words (7803498 effective words) took 10.1s, 772282 effective words/s\n",
      "2019-11-26 22:49:25,638 : INFO : EPOCH 2 - PROGRESS: at 9.55% examples, 738059 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:26,641 : INFO : EPOCH 2 - PROGRESS: at 19.70% examples, 758832 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:27,641 : INFO : EPOCH 2 - PROGRESS: at 30.00% examples, 768800 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:28,644 : INFO : EPOCH 2 - PROGRESS: at 39.49% examples, 765750 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:29,671 : INFO : EPOCH 2 - PROGRESS: at 49.32% examples, 761804 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:30,689 : INFO : EPOCH 2 - PROGRESS: at 59.53% examples, 765089 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:31,703 : INFO : EPOCH 2 - PROGRESS: at 69.28% examples, 763023 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:32,714 : INFO : EPOCH 2 - PROGRESS: at 78.34% examples, 755655 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:33,719 : INFO : EPOCH 2 - PROGRESS: at 88.08% examples, 755610 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:34,729 : INFO : EPOCH 2 - PROGRESS: at 98.30% examples, 759292 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:34,889 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-26 22:49:34,897 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 22:49:34,912 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 22:49:34,915 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 22:49:34,916 : INFO : EPOCH - 2 : training on 10978770 raw words (7804134 effective words) took 10.3s, 759063 effective words/s\n",
      "2019-11-26 22:49:35,938 : INFO : EPOCH 3 - PROGRESS: at 10.02% examples, 774822 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:36,947 : INFO : EPOCH 3 - PROGRESS: at 20.07% examples, 771116 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:37,949 : INFO : EPOCH 3 - PROGRESS: at 30.17% examples, 772103 words/s, in_qsize 8, out_qsize 0\n",
      "2019-11-26 22:49:38,952 : INFO : EPOCH 3 - PROGRESS: at 40.05% examples, 775500 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:39,953 : INFO : EPOCH 3 - PROGRESS: at 49.87% examples, 773401 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:40,966 : INFO : EPOCH 3 - PROGRESS: at 59.97% examples, 774365 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:41,973 : INFO : EPOCH 3 - PROGRESS: at 69.90% examples, 773508 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:42,980 : INFO : EPOCH 3 - PROGRESS: at 79.61% examples, 771263 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:43,983 : INFO : EPOCH 3 - PROGRESS: at 89.75% examples, 773438 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:44,987 : INFO : EPOCH 3 - PROGRESS: at 99.25% examples, 770233 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:45,047 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-26 22:49:45,055 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 22:49:45,064 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 22:49:45,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 22:49:45,073 : INFO : EPOCH - 3 : training on 10978770 raw words (7803163 effective words) took 10.1s, 769864 effective words/s\n",
      "2019-11-26 22:49:46,083 : INFO : EPOCH 4 - PROGRESS: at 9.93% examples, 764529 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:47,087 : INFO : EPOCH 4 - PROGRESS: at 19.98% examples, 767835 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:48,089 : INFO : EPOCH 4 - PROGRESS: at 29.73% examples, 760671 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:49,100 : INFO : EPOCH 4 - PROGRESS: at 39.32% examples, 760279 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:50,102 : INFO : EPOCH 4 - PROGRESS: at 49.32% examples, 763921 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:51,113 : INFO : EPOCH 4 - PROGRESS: at 59.53% examples, 767869 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:52,126 : INFO : EPOCH 4 - PROGRESS: at 69.64% examples, 769342 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:53,127 : INFO : EPOCH 4 - PROGRESS: at 79.16% examples, 766411 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:54,133 : INFO : EPOCH 4 - PROGRESS: at 88.34% examples, 760460 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:55,142 : INFO : EPOCH 4 - PROGRESS: at 98.22% examples, 760968 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:55,296 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-26 22:49:55,301 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 22:49:55,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-11-26 22:49:55,321 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 22:49:55,321 : INFO : EPOCH - 4 : training on 10978770 raw words (7804717 effective words) took 10.2s, 761901 effective words/s\n",
      "2019-11-26 22:49:56,342 : INFO : EPOCH 5 - PROGRESS: at 9.38% examples, 724082 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:57,358 : INFO : EPOCH 5 - PROGRESS: at 19.62% examples, 750065 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:58,359 : INFO : EPOCH 5 - PROGRESS: at 29.63% examples, 755767 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:49:59,378 : INFO : EPOCH 5 - PROGRESS: at 38.91% examples, 747927 words/s, in_qsize 7, out_qsize 1\n",
      "2019-11-26 22:50:00,379 : INFO : EPOCH 5 - PROGRESS: at 48.75% examples, 752725 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:01,381 : INFO : EPOCH 5 - PROGRESS: at 58.54% examples, 753880 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:02,388 : INFO : EPOCH 5 - PROGRESS: at 68.44% examples, 755134 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:03,389 : INFO : EPOCH 5 - PROGRESS: at 78.26% examples, 757360 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:04,402 : INFO : EPOCH 5 - PROGRESS: at 87.53% examples, 752619 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:05,411 : INFO : EPOCH 5 - PROGRESS: at 97.35% examples, 753867 words/s, in_qsize 7, out_qsize 0\n",
      "2019-11-26 22:50:05,648 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-11-26 22:50:05,658 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-11-26 22:50:05,672 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-11-26 22:50:05,683 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-11-26 22:50:05,684 : INFO : EPOCH - 5 : training on 10978770 raw words (7803119 effective words) took 10.3s, 754244 effective words/s\n",
      "2019-11-26 22:50:05,686 : INFO : training on a 54893850 raw words (39018631 effective words) took 51.2s, 762377 effective words/s\n",
      "2019-11-26 22:50:05,690 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-11-26 22:50:05,712 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2019-11-26 22:50:05,713 : INFO : not storing attribute vectors_norm\n",
      "2019-11-26 22:50:05,714 : INFO : not storing attribute cum_table\n",
      "2019-11-26 22:50:05,939 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:50:05.958719Z",
     "start_time": "2019-11-27T06:50:05.945030Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviderickson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('crust', 0.7112777829170227),\n",
       " ('pepperoni', 0.6942993998527527),\n",
       " ('pizzas', 0.669792890548706),\n",
       " ('margherita', 0.6257941126823425),\n",
       " ('calzone', 0.5803310871124268),\n",
       " ('mozzarella', 0.5331249833106995),\n",
       " ('dough', 0.5261613130569458),\n",
       " ('slice', 0.510716438293457),\n",
       " ('lasagna', 0.5067446231842041),\n",
       " ('subs', 0.5036941766738892)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('pizza')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:50:05.981966Z",
     "start_time": "2019-11-27T06:50:05.962669Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviderickson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('waitstaff', 0.5400267839431763),\n",
       " ('staff', 0.45935773849487305),\n",
       " ('hospitality', 0.43323051929473877),\n",
       " ('servers', 0.4167942404747009),\n",
       " ('value', 0.41201937198638916),\n",
       " ('communication', 0.4107080101966858),\n",
       " ('bartenders', 0.40486979484558105),\n",
       " ('experience', 0.39227214455604553),\n",
       " ('consistently', 0.3915942311286926),\n",
       " ('ambience', 0.38009440898895264)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:50:05.994667Z",
     "start_time": "2019-11-27T06:50:05.985352Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviderickson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('terrible', 0.6083077192306519),\n",
       " ('horrible', 0.5717869997024536),\n",
       " ('good', 0.55154949426651),\n",
       " ('poor', 0.5341280698776245),\n",
       " ('awful', 0.5277180671691895),\n",
       " ('disappointing', 0.49991902709007263),\n",
       " ('alright', 0.4686276912689209),\n",
       " ('subpar', 0.4635615348815918),\n",
       " ('acceptable', 0.46211737394332886),\n",
       " ('greatest', 0.44733452796936035)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:50:06.016944Z",
     "start_time": "2019-11-27T06:50:06.000437Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # WV.Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = int(0.)\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for review in reviews:\n",
    "       #\n",
    "       # Print a status message every 1000th review\n",
    "       if counter%1000. == 0.:\n",
    "           print (\"Review %d of %d\" % (counter, len(reviews)))\n",
    "       # \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       counter = counter + 1\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:43.735650Z",
     "start_time": "2019-11-27T06:50:06.019042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviderickson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 100000\n",
      "Review 2000 of 100000\n",
      "Review 3000 of 100000\n",
      "Review 4000 of 100000\n",
      "Review 5000 of 100000\n",
      "Review 6000 of 100000\n",
      "Review 7000 of 100000\n",
      "Review 8000 of 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviderickson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 9000 of 100000\n",
      "Review 10000 of 100000\n",
      "Review 11000 of 100000\n",
      "Review 12000 of 100000\n",
      "Review 13000 of 100000\n",
      "Review 14000 of 100000\n",
      "Review 15000 of 100000\n",
      "Review 16000 of 100000\n",
      "Review 17000 of 100000\n",
      "Review 18000 of 100000\n",
      "Review 19000 of 100000\n",
      "Review 20000 of 100000\n",
      "Review 21000 of 100000\n",
      "Review 22000 of 100000\n",
      "Review 23000 of 100000\n",
      "Review 24000 of 100000\n",
      "Review 25000 of 100000\n",
      "Review 26000 of 100000\n",
      "Review 27000 of 100000\n",
      "Review 28000 of 100000\n",
      "Review 29000 of 100000\n",
      "Review 30000 of 100000\n",
      "Review 31000 of 100000\n",
      "Review 32000 of 100000\n",
      "Review 33000 of 100000\n",
      "Review 34000 of 100000\n",
      "Review 35000 of 100000\n",
      "Review 36000 of 100000\n",
      "Review 37000 of 100000\n",
      "Review 38000 of 100000\n",
      "Review 39000 of 100000\n",
      "Review 40000 of 100000\n",
      "Review 41000 of 100000\n",
      "Review 42000 of 100000\n",
      "Review 43000 of 100000\n",
      "Review 44000 of 100000\n",
      "Review 45000 of 100000\n",
      "Review 46000 of 100000\n",
      "Review 47000 of 100000\n",
      "Review 48000 of 100000\n",
      "Review 49000 of 100000\n",
      "Review 50000 of 100000\n",
      "Review 51000 of 100000\n",
      "Review 52000 of 100000\n",
      "Review 53000 of 100000\n",
      "Review 54000 of 100000\n",
      "Review 55000 of 100000\n",
      "Review 56000 of 100000\n",
      "Review 57000 of 100000\n",
      "Review 58000 of 100000\n",
      "Review 59000 of 100000\n",
      "Review 60000 of 100000\n",
      "Review 61000 of 100000\n",
      "Review 62000 of 100000\n",
      "Review 63000 of 100000\n",
      "Review 64000 of 100000\n",
      "Review 65000 of 100000\n",
      "Review 66000 of 100000\n",
      "Review 67000 of 100000\n",
      "Review 68000 of 100000\n",
      "Review 69000 of 100000\n",
      "Review 70000 of 100000\n",
      "Review 71000 of 100000\n",
      "Review 72000 of 100000\n",
      "Review 73000 of 100000\n",
      "Review 74000 of 100000\n",
      "Review 75000 of 100000\n",
      "Review 76000 of 100000\n",
      "Review 77000 of 100000\n",
      "Review 78000 of 100000\n",
      "Review 79000 of 100000\n",
      "Review 80000 of 100000\n",
      "Review 81000 of 100000\n",
      "Review 82000 of 100000\n",
      "Review 83000 of 100000\n",
      "Review 84000 of 100000\n",
      "Review 85000 of 100000\n",
      "Review 86000 of 100000\n",
      "Review 87000 of 100000\n",
      "Review 88000 of 100000\n",
      "Review 89000 of 100000\n",
      "Review 90000 of 100000\n",
      "Review 91000 of 100000\n",
      "Review 92000 of 100000\n",
      "Review 93000 of 100000\n",
      "Review 94000 of 100000\n",
      "Review 95000 of 100000\n",
      "Review 96000 of 100000\n",
      "Review 97000 of 100000\n",
      "Review 98000 of 100000\n",
      "Review 99000 of 100000\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "clean_reviews = []\n",
    "for review in dfreviews[\"text\"]:\n",
    "    clean_reviews.append( review_to_wordlist( review, \\\n",
    "        remove_stopwords=True ))\n",
    "\n",
    "reviewDataVecs = getAvgFeatureVecs( clean_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:43.743252Z",
     "start_time": "2019-11-27T06:51:43.738853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Clustering of Word2Vec\n",
    "# K means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:43.750824Z",
     "start_time": "2019-11-27T06:51:43.745587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewDataVecs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:45.032050Z",
     "start_time": "2019-11-27T06:51:43.755021Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_data_features has tfidf featues\n",
    "# Add other data to each review (stars, useful, ...)\n",
    "# GroupBy User and avg across each review\n",
    "# Cluster users using K-means\n",
    "# Interpret user clusters\n",
    "\n",
    "\n",
    "# Add non-text data back to feature matrix\n",
    "review_features = ['cool', 'funny', 'useful', 'stars' , 'user_id']\n",
    "all_features_labels = ['w2v{}'.format(idx) for idx in range(reviewDataVecs.shape[1])] + review_features\n",
    "all_features = np.append(reviewDataVecs, dfreviews[review_features].to_numpy(), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:59.858962Z",
     "start_time": "2019-11-27T06:51:45.034309Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create df \n",
    "all_features_df = pd.DataFrame(data=all_features, columns=all_features_labels)\n",
    "all_features_df = all_features_df.iloc[:,:-1].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:51:59.882209Z",
     "start_time": "2019-11-27T06:51:59.860489Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v0</th>\n",
       "      <th>w2v1</th>\n",
       "      <th>w2v2</th>\n",
       "      <th>w2v3</th>\n",
       "      <th>w2v4</th>\n",
       "      <th>w2v5</th>\n",
       "      <th>w2v6</th>\n",
       "      <th>w2v7</th>\n",
       "      <th>w2v8</th>\n",
       "      <th>w2v9</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v294</th>\n",
       "      <th>w2v295</th>\n",
       "      <th>w2v296</th>\n",
       "      <th>w2v297</th>\n",
       "      <th>w2v298</th>\n",
       "      <th>w2v299</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029347</td>\n",
       "      <td>-0.045292</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>-0.056094</td>\n",
       "      <td>-0.000239</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.020766</td>\n",
       "      <td>-0.008233</td>\n",
       "      <td>0.011725</td>\n",
       "      <td>-0.027301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002513</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.042669</td>\n",
       "      <td>-0.025909</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.017668</td>\n",
       "      <td>0.015019</td>\n",
       "      <td>0.005176</td>\n",
       "      <td>0.015874</td>\n",
       "      <td>-0.005444</td>\n",
       "      <td>0.002493</td>\n",
       "      <td>-0.020803</td>\n",
       "      <td>-0.014717</td>\n",
       "      <td>-0.008088</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014005</td>\n",
       "      <td>0.012350</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>-0.023010</td>\n",
       "      <td>-0.006996</td>\n",
       "      <td>-0.015238</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003682</td>\n",
       "      <td>-0.013629</td>\n",
       "      <td>-0.013201</td>\n",
       "      <td>-0.021520</td>\n",
       "      <td>-0.017956</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>-0.035326</td>\n",
       "      <td>-0.043122</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>-0.023320</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010395</td>\n",
       "      <td>0.017004</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>-0.025227</td>\n",
       "      <td>-0.009671</td>\n",
       "      <td>-0.042948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.015540</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>0.035326</td>\n",
       "      <td>0.025240</td>\n",
       "      <td>-0.025267</td>\n",
       "      <td>-0.020772</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.010747</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.050980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017691</td>\n",
       "      <td>0.009782</td>\n",
       "      <td>0.018971</td>\n",
       "      <td>-0.037009</td>\n",
       "      <td>-0.019171</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.006893</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>-0.015601</td>\n",
       "      <td>-0.019284</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>-0.011306</td>\n",
       "      <td>-0.023552</td>\n",
       "      <td>-0.016073</td>\n",
       "      <td>0.027782</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011642</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.006393</td>\n",
       "      <td>0.008172</td>\n",
       "      <td>-0.001957</td>\n",
       "      <td>-0.015563</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       w2v0      w2v1      w2v2      w2v3      w2v4      w2v5      w2v6  \\\n",
       "0  0.029347 -0.045292 -0.003437 -0.056094 -0.000239 -0.000536 -0.020766   \n",
       "1 -0.017668  0.015019  0.005176  0.015874 -0.005444  0.002493 -0.020803   \n",
       "2  0.003682 -0.013629 -0.013201 -0.021520 -0.017956  0.014747 -0.035326   \n",
       "3 -0.015540  0.000759  0.035326  0.025240 -0.025267 -0.020772  0.001695   \n",
       "4  0.006893  0.007568 -0.015601 -0.019284 -0.006614 -0.011306 -0.023552   \n",
       "\n",
       "       w2v7      w2v8      w2v9  ...    w2v294    w2v295    w2v296    w2v297  \\\n",
       "0 -0.008233  0.011725 -0.027301  ... -0.002513  0.002455  0.003002  0.042669   \n",
       "1 -0.014717 -0.008088  0.000974  ... -0.014005  0.012350  0.012323 -0.023010   \n",
       "2 -0.043122  0.003569 -0.023320  ... -0.010395  0.017004  0.015000 -0.025227   \n",
       "3  0.010747  0.003403  0.050980  ... -0.017691  0.009782  0.018971 -0.037009   \n",
       "4 -0.016073  0.027782 -0.000110  ... -0.011642  0.008734  0.006393  0.008172   \n",
       "\n",
       "     w2v298    w2v299  cool  funny  useful  stars  \n",
       "0 -0.025909 -0.001635   0.0    1.0     6.0    1.0  \n",
       "1 -0.006996 -0.015238   0.0    0.0     0.0    5.0  \n",
       "2 -0.009671 -0.042948   0.0    0.0     3.0    5.0  \n",
       "3 -0.019171  0.018166   0.0    0.0     0.0    5.0  \n",
       "4 -0.001957 -0.015563   0.0    0.0     7.0    1.0  \n",
       "\n",
       "[5 rows x 304 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:52:01.723307Z",
     "start_time": "2019-11-27T06:51:59.884719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v0</th>\n",
       "      <th>w2v1</th>\n",
       "      <th>w2v2</th>\n",
       "      <th>w2v3</th>\n",
       "      <th>w2v4</th>\n",
       "      <th>w2v5</th>\n",
       "      <th>w2v6</th>\n",
       "      <th>w2v7</th>\n",
       "      <th>w2v8</th>\n",
       "      <th>w2v9</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v294</th>\n",
       "      <th>w2v295</th>\n",
       "      <th>w2v296</th>\n",
       "      <th>w2v297</th>\n",
       "      <th>w2v298</th>\n",
       "      <th>w2v299</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>99986.000000</td>\n",
       "      <td>100000.00000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "      <td>100000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.007101</td>\n",
       "      <td>-0.005923</td>\n",
       "      <td>0.013527</td>\n",
       "      <td>0.009325</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>-0.003560</td>\n",
       "      <td>-0.002984</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>0.014403</td>\n",
       "      <td>0.020219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005905</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.54412</td>\n",
       "      <td>0.442580</td>\n",
       "      <td>1.279980</td>\n",
       "      <td>3.736920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.012419</td>\n",
       "      <td>0.022992</td>\n",
       "      <td>0.026325</td>\n",
       "      <td>0.019279</td>\n",
       "      <td>0.018700</td>\n",
       "      <td>0.022606</td>\n",
       "      <td>0.023564</td>\n",
       "      <td>0.017968</td>\n",
       "      <td>0.022780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013176</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.015345</td>\n",
       "      <td>0.022285</td>\n",
       "      <td>0.014069</td>\n",
       "      <td>0.019864</td>\n",
       "      <td>2.04579</td>\n",
       "      <td>1.648487</td>\n",
       "      <td>2.919489</td>\n",
       "      <td>1.454589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.185167</td>\n",
       "      <td>-0.142547</td>\n",
       "      <td>-0.126197</td>\n",
       "      <td>-0.118765</td>\n",
       "      <td>-0.154010</td>\n",
       "      <td>-0.130265</td>\n",
       "      <td>-0.166816</td>\n",
       "      <td>-0.138313</td>\n",
       "      <td>-0.084816</td>\n",
       "      <td>-0.091574</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.140078</td>\n",
       "      <td>-0.087361</td>\n",
       "      <td>-0.140014</td>\n",
       "      <td>-0.189829</td>\n",
       "      <td>-0.128395</td>\n",
       "      <td>-0.100321</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.020779</td>\n",
       "      <td>-0.013478</td>\n",
       "      <td>-0.003222</td>\n",
       "      <td>-0.009633</td>\n",
       "      <td>-0.012142</td>\n",
       "      <td>-0.015628</td>\n",
       "      <td>-0.018582</td>\n",
       "      <td>-0.012174</td>\n",
       "      <td>0.002693</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013017</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>-0.001045</td>\n",
       "      <td>-0.014711</td>\n",
       "      <td>-0.010847</td>\n",
       "      <td>-0.007393</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.006977</td>\n",
       "      <td>-0.005604</td>\n",
       "      <td>0.012256</td>\n",
       "      <td>0.007801</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>-0.004071</td>\n",
       "      <td>-0.001912</td>\n",
       "      <td>0.005076</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.022220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005571</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.008662</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>-0.001876</td>\n",
       "      <td>0.006918</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.001930</td>\n",
       "      <td>0.029038</td>\n",
       "      <td>0.026912</td>\n",
       "      <td>0.012797</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.020622</td>\n",
       "      <td>0.026244</td>\n",
       "      <td>0.036682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001776</td>\n",
       "      <td>0.011839</td>\n",
       "      <td>0.018222</td>\n",
       "      <td>0.014086</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>0.018248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.176629</td>\n",
       "      <td>0.099597</td>\n",
       "      <td>0.138342</td>\n",
       "      <td>0.146654</td>\n",
       "      <td>0.101255</td>\n",
       "      <td>0.128908</td>\n",
       "      <td>0.107769</td>\n",
       "      <td>0.128541</td>\n",
       "      <td>0.108174</td>\n",
       "      <td>0.137366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103328</td>\n",
       "      <td>0.115160</td>\n",
       "      <td>0.111133</td>\n",
       "      <td>0.135384</td>\n",
       "      <td>0.137527</td>\n",
       "      <td>0.110932</td>\n",
       "      <td>94.00000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               w2v0          w2v1          w2v2          w2v3          w2v4  \\\n",
       "count  99986.000000  99986.000000  99986.000000  99986.000000  99986.000000   \n",
       "mean      -0.007101     -0.005923      0.013527      0.009325      0.000060   \n",
       "std        0.021416      0.012419      0.022992      0.026325      0.019279   \n",
       "min       -0.185167     -0.142547     -0.126197     -0.118765     -0.154010   \n",
       "25%       -0.020779     -0.013478     -0.003222     -0.009633     -0.012142   \n",
       "50%       -0.006977     -0.005604      0.012256      0.007801      0.000739   \n",
       "75%        0.006662      0.001930      0.029038      0.026912      0.012797   \n",
       "max        0.176629      0.099597      0.138342      0.146654      0.101255   \n",
       "\n",
       "               w2v5          w2v6          w2v7          w2v8          w2v9  \\\n",
       "count  99986.000000  99986.000000  99986.000000  99986.000000  99986.000000   \n",
       "mean      -0.003560     -0.002984      0.004399      0.014403      0.020219   \n",
       "std        0.018700      0.022606      0.023564      0.017968      0.022780   \n",
       "min       -0.130265     -0.166816     -0.138313     -0.084816     -0.091574   \n",
       "25%       -0.015628     -0.018582     -0.012174      0.002693      0.002521   \n",
       "50%       -0.004071     -0.001912      0.005076      0.014648      0.022220   \n",
       "75%        0.007993      0.013100      0.020622      0.026244      0.036682   \n",
       "max        0.128908      0.107769      0.128541      0.108174      0.137366   \n",
       "\n",
       "       ...        w2v294        w2v295        w2v296        w2v297  \\\n",
       "count  ...  99986.000000  99986.000000  99986.000000  99986.000000   \n",
       "mean   ...     -0.005905      0.003377      0.008631     -0.000969   \n",
       "std    ...      0.013176      0.013889      0.015345      0.022285   \n",
       "min    ...     -0.140078     -0.087361     -0.140014     -0.189829   \n",
       "25%    ...     -0.013017     -0.005517     -0.001045     -0.014711   \n",
       "50%    ...     -0.005571      0.002972      0.008662      0.000377   \n",
       "75%    ...      0.001776      0.011839      0.018222      0.014086   \n",
       "max    ...      0.103328      0.115160      0.111133      0.135384   \n",
       "\n",
       "             w2v298        w2v299          cool          funny         useful  \\\n",
       "count  99986.000000  99986.000000  100000.00000  100000.000000  100000.000000   \n",
       "mean      -0.002034      0.004596       0.54412       0.442580       1.279980   \n",
       "std        0.014069      0.019864       2.04579       1.648487       2.919489   \n",
       "min       -0.128395     -0.100321       0.00000       0.000000       0.000000   \n",
       "25%       -0.010847     -0.007393       0.00000       0.000000       0.000000   \n",
       "50%       -0.001876      0.006918       0.00000       0.000000       0.000000   \n",
       "75%        0.006841      0.018248       0.00000       0.000000       1.000000   \n",
       "max        0.137527      0.110932      94.00000      74.000000     101.000000   \n",
       "\n",
       "               stars  \n",
       "count  100000.000000  \n",
       "mean        3.736920  \n",
       "std         1.454589  \n",
       "min         1.000000  \n",
       "25%         3.000000  \n",
       "50%         4.000000  \n",
       "75%         5.000000  \n",
       "max         5.000000  \n",
       "\n",
       "[8 rows x 304 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-27T06:59:12.753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster users using K-means\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "max_clusters = 100 # 10\n",
    "kmeans_cost = []\n",
    "for num_clusters in range(1,max_clusters):\n",
    "    k_means_clutering = KMeans(n_clusters=num_clusters)\n",
    "    k_means_clutering.fit(reviewDataVecs[~np.isnan(reviewDataVecs).any(axis=1)]) # Drop rows that have NAN\n",
    "    kmeans_cost.append(k_means_clutering.inertia_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-27T06:59:13.661Z"
    }
   },
   "outputs": [],
   "source": [
    "# Determine the best value of K to use (the number of clusters)\n",
    "# plot the cost against K values \n",
    "plt.plot(range(1, max_clusters), kmeans_cost, color ='g', linewidth ='3') \n",
    "plt.title('4 Clusters of Reviews:', fontsize=20)\n",
    "plt.xlabel(\"Value of K\") \n",
    "plt.ylabel(\"Sqaured Error (Cost)\") \n",
    "plt.show() # clear the plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-27T06:59:15.008Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cluster users using K-means\n",
    "# Interpret user clusters\n",
    "\n",
    "num_clusters = 4\n",
    "k_means_clutering = KMeans(n_clusters=num_clusters)\n",
    "k_means_clutering.fit(reviewDataVecs[~np.isnan(reviewDataVecs).any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:56:28.003714Z",
     "start_time": "2019-11-27T06:56:27.964244Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v0</th>\n",
       "      <th>w2v1</th>\n",
       "      <th>w2v2</th>\n",
       "      <th>w2v3</th>\n",
       "      <th>w2v4</th>\n",
       "      <th>w2v5</th>\n",
       "      <th>w2v6</th>\n",
       "      <th>w2v7</th>\n",
       "      <th>w2v8</th>\n",
       "      <th>w2v9</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v290</th>\n",
       "      <th>w2v291</th>\n",
       "      <th>w2v292</th>\n",
       "      <th>w2v293</th>\n",
       "      <th>w2v294</th>\n",
       "      <th>w2v295</th>\n",
       "      <th>w2v296</th>\n",
       "      <th>w2v297</th>\n",
       "      <th>w2v298</th>\n",
       "      <th>w2v299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003666</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>-0.005807</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.024388</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002131</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.008992</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>-0.005642</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.007684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>-0.004359</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>-0.020827</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>-0.004078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009187</td>\n",
       "      <td>0.006237</td>\n",
       "      <td>-0.022233</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>-0.005268</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>-0.014992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005629</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>0.016246</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>-0.004047</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>-0.002837</td>\n",
       "      <td>-0.015496</td>\n",
       "      <td>-0.010019</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>0.004493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.018410</td>\n",
       "      <td>-0.007184</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>-0.015262</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.026107</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.036187</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011441</td>\n",
       "      <td>-0.012380</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.005723</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>-0.004542</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>0.017721</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       w2v0      w2v1      w2v2      w2v3      w2v4      w2v5      w2v6  \\\n",
       "0 -0.003666 -0.005392  0.004432  0.001858  0.003041 -0.002559 -0.005807   \n",
       "1  0.001386 -0.004426 -0.004359 -0.013932 -0.013383  0.002972 -0.020827   \n",
       "2 -0.005629 -0.006665  0.016246  0.006963 -0.008483  0.004568 -0.004047   \n",
       "3 -0.018410 -0.007184  0.035339  0.037301  0.013807 -0.015262  0.015137   \n",
       "\n",
       "       w2v7      w2v8      w2v9  ...    w2v290    w2v291    w2v292    w2v293  \\\n",
       "0  0.000002  0.024388  0.015880  ... -0.002131 -0.000720 -0.008992  0.000255   \n",
       "1 -0.018214  0.019324 -0.004078  ... -0.009187  0.006237 -0.022233  0.006587   \n",
       "2  0.006174  0.013998  0.032717  ... -0.002149 -0.002837 -0.015496 -0.010019   \n",
       "3  0.026107  0.000839  0.036187  ...  0.011441 -0.012380 -0.000084 -0.005723   \n",
       "\n",
       "     w2v294    w2v295    w2v296    w2v297    w2v298    w2v299  \n",
       "0 -0.005642  0.003715  0.004280  0.007932  0.000606  0.007684  \n",
       "1 -0.005268  0.009619  0.015624  0.000312  0.002547 -0.014992  \n",
       "2 -0.009113  0.007113  0.006697 -0.018495 -0.000367  0.004493  \n",
       "3 -0.004572 -0.004542  0.008429  0.000803 -0.009482  0.017721  \n",
       "\n",
       "[4 rows x 300 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_centers_df = pd.DataFrame(data=k_means_clutering.cluster_centers_, columns=all_features_df.iloc[:,:-4].columns)\n",
    "cluster_centers_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:56:29.666724Z",
     "start_time": "2019-11-27T06:56:28.006348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w2v0</th>\n",
       "      <th>w2v1</th>\n",
       "      <th>w2v2</th>\n",
       "      <th>w2v3</th>\n",
       "      <th>w2v4</th>\n",
       "      <th>w2v5</th>\n",
       "      <th>w2v6</th>\n",
       "      <th>w2v7</th>\n",
       "      <th>w2v8</th>\n",
       "      <th>w2v9</th>\n",
       "      <th>...</th>\n",
       "      <th>w2v294</th>\n",
       "      <th>w2v295</th>\n",
       "      <th>w2v296</th>\n",
       "      <th>w2v297</th>\n",
       "      <th>w2v298</th>\n",
       "      <th>w2v299</th>\n",
       "      <th>cool</th>\n",
       "      <th>funny</th>\n",
       "      <th>useful</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003666</td>\n",
       "      <td>-0.005392</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>-0.002559</td>\n",
       "      <td>-0.005807</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.024388</td>\n",
       "      <td>0.015880</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005642</td>\n",
       "      <td>0.003715</td>\n",
       "      <td>0.004280</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.007684</td>\n",
       "      <td>0.733548</td>\n",
       "      <td>0.674057</td>\n",
       "      <td>1.485736</td>\n",
       "      <td>3.325766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>-0.004359</td>\n",
       "      <td>-0.013932</td>\n",
       "      <td>-0.013383</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>-0.020827</td>\n",
       "      <td>-0.018214</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>-0.004078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005268</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>0.002547</td>\n",
       "      <td>-0.014992</td>\n",
       "      <td>0.360401</td>\n",
       "      <td>0.381904</td>\n",
       "      <td>1.730795</td>\n",
       "      <td>3.498398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.005629</td>\n",
       "      <td>-0.006665</td>\n",
       "      <td>0.016246</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>-0.008483</td>\n",
       "      <td>0.004568</td>\n",
       "      <td>-0.004047</td>\n",
       "      <td>0.006174</td>\n",
       "      <td>0.013998</td>\n",
       "      <td>0.032717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009113</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>-0.000367</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.300877</td>\n",
       "      <td>0.157630</td>\n",
       "      <td>0.561230</td>\n",
       "      <td>4.469634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.018410</td>\n",
       "      <td>-0.007184</td>\n",
       "      <td>0.035339</td>\n",
       "      <td>0.037301</td>\n",
       "      <td>0.013807</td>\n",
       "      <td>-0.015262</td>\n",
       "      <td>0.015137</td>\n",
       "      <td>0.026107</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.036187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004572</td>\n",
       "      <td>-0.004542</td>\n",
       "      <td>0.008429</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>-0.009482</td>\n",
       "      <td>0.017721</td>\n",
       "      <td>0.669851</td>\n",
       "      <td>0.453535</td>\n",
       "      <td>1.182570</td>\n",
       "      <td>3.852421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 304 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       w2v0      w2v1      w2v2      w2v3      w2v4      w2v5      w2v6  \\\n",
       "0 -0.003666 -0.005392  0.004432  0.001858  0.003041 -0.002559 -0.005807   \n",
       "1  0.001386 -0.004426 -0.004359 -0.013932 -0.013383  0.002972 -0.020827   \n",
       "2 -0.005629 -0.006665  0.016246  0.006963 -0.008483  0.004568 -0.004047   \n",
       "3 -0.018410 -0.007184  0.035339  0.037301  0.013807 -0.015262  0.015137   \n",
       "\n",
       "       w2v7      w2v8      w2v9  ...    w2v294    w2v295    w2v296    w2v297  \\\n",
       "0  0.000002  0.024388  0.015880  ... -0.005642  0.003715  0.004280  0.007932   \n",
       "1 -0.018214  0.019324 -0.004078  ... -0.005268  0.009619  0.015624  0.000312   \n",
       "2  0.006174  0.013998  0.032717  ... -0.009113  0.007113  0.006697 -0.018495   \n",
       "3  0.026107  0.000839  0.036187  ... -0.004572 -0.004542  0.008429  0.000803   \n",
       "\n",
       "     w2v298    w2v299      cool     funny    useful     stars  \n",
       "0  0.000606  0.007684  0.733548  0.674057  1.485736  3.325766  \n",
       "1  0.002547 -0.014992  0.360401  0.381904  1.730795  3.498398  \n",
       "2 -0.000367  0.004493  0.300877  0.157630  0.561230  4.469634  \n",
       "3 -0.009482  0.017721  0.669851  0.453535  1.182570  3.852421  \n",
       "\n",
       "[4 rows x 304 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "cluster_centers_df.merge(right=all_features_df.dropna()[['cool', 'funny', 'useful', 'stars']].groupby(by=k_means_clutering.labels_).mean(), right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T06:56:29.771939Z",
     "start_time": "2019-11-27T06:56:29.669652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most similar words to the avg vector describing each user cluster:\n",
      "\n",
      "Cluster 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('church', 0.47782468795776367),\n",
       " ('norm', 0.44951778650283813),\n",
       " ('place', 0.43114006519317627),\n",
       " ('crowded', 0.4275950789451599),\n",
       " ('crowd', 0.4227985441684723),\n",
       " ('seats', 0.4089027941226959),\n",
       " ('hotel', 0.4078960120677948),\n",
       " ('casino', 0.407378226518631),\n",
       " ('restaurant', 0.3996233344078064),\n",
       " ('lounge', 0.39664745330810547)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('diagnosis', 0.6463435292243958),\n",
       " ('loan', 0.6387569904327393),\n",
       " ('project', 0.6233497262001038),\n",
       " ('scheduling', 0.6231913566589355),\n",
       " ('computer', 0.6164991855621338),\n",
       " ('property', 0.6109764575958252),\n",
       " ('vehicle', 0.6102975606918335),\n",
       " ('client', 0.6080654859542847),\n",
       " ('crew', 0.6047731637954712),\n",
       " ('camera', 0.599032998085022)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('great', 0.6011333465576172),\n",
       " ('ambiance', 0.5823598504066467),\n",
       " ('atmosphere', 0.5792982578277588),\n",
       " ('good', 0.5688526630401611),\n",
       " ('food', 0.5605561137199402),\n",
       " ('ambience', 0.5578678846359253),\n",
       " ('quaint', 0.5352204442024231),\n",
       " ('casual', 0.5344662666320801),\n",
       " ('satisfying', 0.5232836008071899),\n",
       " ('inexpensive', 0.5232647061347961)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('panini', 0.696311354637146),\n",
       " ('tasty', 0.6772598028182983),\n",
       " ('delish', 0.6740022897720337),\n",
       " ('samosas', 0.6630558967590332),\n",
       " ('flavorful', 0.6623433232307434),\n",
       " ('empanadas', 0.6512776613235474),\n",
       " ('cabbage', 0.6481821537017822),\n",
       " ('eggplant', 0.6469069719314575),\n",
       " ('broccoli', 0.6464052200317383),\n",
       " ('jalapeno', 0.6421990394592285)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('The most similar words to the avg vector describing each user cluster:\\n')\n",
    "for cluster in range(len(cluster_centers_df)):\n",
    "    print('Cluster {}'.format(cluster))\n",
    "    display(\n",
    "        model.wv.similar_by_vector(cluster_centers_df.iloc[cluster,:].values, topn=10)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-11-27T07:06:28.532Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "for cluster in range(len(cluster_centers_df)):\n",
    "#     series = user_cluster_centers_df.iloc[cluster,:-4] #Use relevant row, drop non-word cols\n",
    "#     wc_dict = series.to_dict()\n",
    "    series = model.wv.similar_by_vector(cluster_centers_df.iloc[cluster,:].values, topn=100)\n",
    "    wc_dict = {}\n",
    "    for key, val in series:\n",
    "        wc_dict[key] = val\n",
    "    wordcloud = WordCloud().generate_from_frequencies(wc_dict)\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title('Cluster {}: {}stars, {}useful'.format(cluster, cluster_centers_df.iloc[cluster]['stars'], \n",
    "                                                    cluster_centers_df.iloc[cluster]['useful']), fontsize=20)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
